{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_Keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_SREcJunL7-",
        "colab_type": "text"
      },
      "source": [
        "<h1>\n",
        "TensorFlowのKerasを使って顔認識してみる\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQKbvNSQDPSB",
        "colab_type": "text"
      },
      "source": [
        "<h1>\n",
        "<span id=\"ステップ２google-colaboratoryの準備\" class=\"fragment\"></span>・Google Colaboratoryの準備</h1>\n",
        "\n",
        "<p>無料でGPUを使うために、Google Colaboratoryを利用します。<br>\n",
        "Google Colaboratoryの使い方は下記を参考に。<br>\n",
        "<a href=\"https://www.codexa.net/how-to-use-google-colaboratory/\" rel=\"nofollow noopener\" target=\"_blank\">Google Colabの知っておくべき使い方</a></p>\n",
        "\n",
        "<h3>\n",
        "<span id=\"gpuを設定\" class=\"fragment\"></span>・GPUを設定</h3>\n",
        "\n",
        "<p>画面上部のメニュー<br>\n",
        " <code>編集</code> &gt; <code>ノートブックの設定</code>&gt;<code>ハードウェア　アクセラレータ</code>&gt;<code>GPU</code>に変更して保存する。<br>\n",
        "<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F75534cbc-6fc1-2e33-31b3-46ed6b5412df.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=600fdad80b17516a02b32a0f21550b8c\" target=\"_blank\" rel=\"nofollow noopener\"><img width=\"1372\" alt=\"機械学習_-_Colaboratory.png\" src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F75534cbc-6fc1-2e33-31b3-46ed6b5412df.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=600fdad80b17516a02b32a0f21550b8c\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/322279/75534cbc-6fc1-2e33-31b3-46ed6b5412df.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F75534cbc-6fc1-2e33-31b3-46ed6b5412df.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=df70179900f4e257d95e6c1054f29651 1x\" loading=\"lazy\"></a></p>\n",
        "\n",
        "<p><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F5b5d945b-440e-109a-1c56-a8a9fa73c4e6.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=010805d3cd0b0fe94174d209e6bd5bb5\" target=\"_blank\" rel=\"nofollow noopener\"><img width=\"521\" alt=\"機械学習_-_Colaboratory-2.png\" src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F5b5d945b-440e-109a-1c56-a8a9fa73c4e6.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=010805d3cd0b0fe94174d209e6bd5bb5\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/322279/5b5d945b-440e-109a-1c56-a8a9fa73c4e6.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F5b5d945b-440e-109a-1c56-a8a9fa73c4e6.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=1d4d09cce8ca34c799f4c97d76a30711 1x\" loading=\"lazy\"></a></p>\n",
        "\n",
        "<h3>\n",
        "<span id=\"gpuの割り当てを確認\" class=\"fragment\"></span>・GPUの割り当てを確認</h3>\n",
        "\n",
        "<p>下記のように出力されていれば、GPUが割り当てられています。</p>\n",
        "\n",
        "<div class=\"code-frame\" data-lang=\"text\"><div class=\"highlight\"><pre>'/device:GPU:0'\n",
        "</pre></div></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct3sKmJOF3uC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<p>・下記のセルを実行して確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6R-_LlFPCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk1XzNyfF1Kp",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"google-driveに画像データをあげる\" class=\"fragment\"></span>・Google Driveに画像データをあげる</h3>\n",
        "\n",
        "<p>colaboratoryを利用して、学習を行うには、Google driveに画像データをアップする必要があります。<br>\n",
        "そして、Google Driveへのマウントをすることで、colaboratoryからGoogle driveにあるデータを参照できるようになります。</p>\n",
        "\n",
        "＊今回はAWS S3からデータのダウウンロードしてくる<br>\n",
        "＊haarcascadesは顔検出に用いる。GitHubからダウウンロードしてくる<br><br>\n",
        "\n",
        "<p><a href=\"https://ediotwebserver.web.app/fileserver/Gdrive_dirs_colabratory.PNG\" target=\"_blank\" rel=\"nofollow noopener\"><img width=\"1133\" alt=\"Colab_Notebooks_-_Google_ドライブ.png\" src=\"https://ediotwebserver.web.app/fileserver/Gdrive_dirs_colabratory.PNG\" data-canonical-src=\"https://ediotwebserver.web.app/fileserver/Gdrive_dirs_colabratory.PNG\" srcset=\"https://ediotwebserver.web.app/fileserver/Gdrive_dirs_colabratory.PNG\" loading=\"lazy\"></a></p>\n",
        "\n",
        "<p>ディレクトリ構造を下記に示します。</p>\n",
        "\n",
        "<blockquote>\n",
        "<p>content<br>\n",
        "　├　gdrive<br>\n",
        "　　　└ My Drive<br>\n",
        "　　　　　└ Colab Notebooks<br>\n",
        "　　　　　　　├ \"GroupName\"<br>\n",
        "　　　　　　　│　　└facedetect　(＊S3からダウウンロード）<br>　\n",
        "　　　　　　　│　　　　└ 名前１<br>\n",
        "　　　　　　　│　　　　└ 名前２<br>\n",
        "　　　　　　　│　　　　└ 名前３<br>\n",
        "　　　　　　　├　haarcascades　（＊GitHubからダウウンロード）<br>\n",
        "　　　　　　　　　　└ haarcascade_frontalface_alt.xml<br>\n",
        "\n",
        "</blockquote>\n",
        "<br>\n",
        "<p>自分のディレクトリの確認をしたいときは、Linuxの基本コマンドを使用。<br>\n",
        "あとでトレーニングデータとテストデータのパスを取得する時に必要になる。</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrUX22GFGsnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#カレントディレクトリを確認\n",
        "!pwd\n",
        "\n",
        "#ディレクトリの内容を表示\n",
        "!ls\n",
        "\n",
        "#ディレクトリを移動\n",
        "%cd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqRaBs13HMoZ",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"google-driveをマウント\" class=\"fragment\"></span>・Google Driveをマウント</h3>\n",
        "\n",
        "<p>以下のコードで、content /gdrive /My Drive/のなかにgoogle driveのデータが入ります</p>\n",
        "\n",
        "<blockquote>\n",
        "<p>content<br>\n",
        "　├ gdrive<br>\n",
        "　　└ My Drive</p>\n",
        "</blockquote>\n",
        "\n",
        "\n",
        "<p>下記を実行すると、URLと認証コードの入力フォームが表示されます。<br>\n",
        "URLをクリックして、googleアカウントの認証コードを取得してください。<br>\n",
        "認証コードをコピーして、入力フォームに貼り付けると、content /gdrive/ My Drive/のなかにgoogle driveのデータが入ります。</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrz0VhfdHIU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%pwd\n",
        "\n",
        "%cd /content/gdrive/'My Drive'/Colab Notebooks\n",
        "\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmw43TLLjxmW",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"google-driveをマウント\" class=\"fragment\"></span>・データをダウンロード 　＊編集</h3>\n",
        "\n",
        "aws S3 から　\"groupname\"/facedetectに保存されたデータをダウンロードしてくる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4hpMuwjBi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 -*-\n",
        "import logging\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import os\n",
        "\n",
        "groupname = \"YOUR_GROUP_NAME\" # グループ名を入力\n",
        "groupdir = \"/content/gdrive/My Drive/Colab Notebooks/\" + groupname\n",
        "\n",
        "try:\n",
        "    bucketname = \"ediot\"\n",
        "    KeyPrefix = groupname + '/facedetect'\n",
        "    resource = boto3.resource(\n",
        "        's3',\n",
        "        region_name='ap-northeast-1',\n",
        "        aws_access_key_id='YOUR_ACCESS_KEY', # AWS のアクセスキー\n",
        "        aws_secret_access_key='YOUR_SECRET_KEY' #　AWS のシークレットキー\n",
        "    )\n",
        "except ClientError as e:\n",
        "        logging.error(e)\n",
        "        \n",
        "bucket = resource.Bucket(bucketname)\n",
        "objs = bucket.meta.client.list_objects_v2(Bucket=bucket.name, Prefix=KeyPrefix)\n",
        "for o in objs.get('Contents'):\n",
        "    key = o.get('Key')\n",
        "    if key[-4:]=='.jpg':\n",
        "        print(key)\n",
        "        os.makedirs(\"/content/gdrive/My Drive/Colab Notebooks/\" + os.path.dirname(key), exist_ok=True)\n",
        "        bucket.download_file(key, \"/content/gdrive/My Drive/Colab Notebooks/\" + key) # download\n",
        "\n",
        "#download\n",
        "#bucket.upload_file('DownloadするS3のpath', '保存先のpath')\n",
        "\n",
        "#upload\n",
        "#bucket.upload_file('UPするファイルのpath', '保存先S3のpath')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzkUne7yoyqb",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "・haarcascadesのダウウンロード</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr06Snvso9jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/gdrive/'My Drive'/Colab Notebooks\n",
        "%pwd\n",
        "\n",
        "!git clone https://github.com/2DD4nVn8/haarcascades.git\n",
        "\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBVpTOAhIJnU",
        "colab_type": "text"
      },
      "source": [
        "<h1>・データの前処理</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG1v4fZcPGH6",
        "colab_type": "text"
      },
      "source": [
        "<h2>\n",
        "<span id=\"ステップ３顔部分を抽出し切り抜く\" class=\"fragment\"></span>・顔部分を抽出し切り抜く</h2>\n",
        "\n",
        "<p>顔部分を抽出するのに、OpneCVを利用します。<br>\n",
        "下記のgithubから、OpneCVをダウンロードできる。<br>\n",
        "<a href=\"https://github.com/opencv/opencv\" rel=\"nofollow noopener\" target=\"_blank\">OpenCV</a></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNuk9ynPIe3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# OpenCVのデフォルトの分類器のpath。(https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_alt.xmlのファイルを使う)\n",
        "cascade_path = \"/content/gdrive/My Drive/Colab Notebooks/haarcascades/haarcascade_frontalface_alt.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(cascade_path)\n",
        "\n",
        "input_dir = groupdir + \"/facedetect/\"\n",
        "FaceName =  os.listdir(input_dir)\n",
        "print(FaceName)\n",
        "\n",
        "for name in FaceName:   \n",
        "    # 画像データのあるディレクトリ\n",
        "    input_data_path = input_dir + str(name)\n",
        "    # 切り抜いた画像の保存先ディレクトリを作成\n",
        "    os.makedirs(groupdir + \"/face/\"+str(name), exist_ok=True)\n",
        "    save_path = groupdir + \"/face/\"+str(name)\n",
        "    # 収集した画像の枚数(任意で変更)\n",
        "    # 顔検知に成功した数(デフォルトで0を指定)\n",
        "    face_detect_count = 0\n",
        "\n",
        "    print(\"{}の顔を検出し切り取りを開始します。\".format(name))\n",
        "    # 集めた画像データから顔が検知されたら、切り取り、保存する。\n",
        "\n",
        "    files =  os.listdir(input_data_path)\n",
        "    print(files)\n",
        "\n",
        "    for file_name in files:\n",
        "#    for i in range(image_count):\n",
        "        img = cv2.imread(input_data_path + \"/\" + file_name)\n",
        "        if img is None:\n",
        "             print('image : ' + input_data_path + \"/\" + file_name + ':NoImage')\n",
        "        else:\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            face = faceCascade.detectMultiScale(gray, 1.1, 3)\n",
        "            if len(face) > 0:\n",
        "                for rect in face:\n",
        "                    # 顔認識部分を赤線で囲み保存(今はこの部分は必要ない)\n",
        "                    # cv2.rectangle(img, tuple(rect[0:2]), tuple(rect[0:2]+rect[2:4]), (0, 0,255), thickness=1)\n",
        "                    # cv2.imwrite('detected.jpg', img)\n",
        "                    x = rect[0]\n",
        "                    y = rect[1]\n",
        "                    w = rect[2]\n",
        "                    h = rect[3]\n",
        "                    cv2.imwrite(save_path + '/cutted' + str(face_detect_count) + '.jpg',img[y:y+h,  x:x+w])\n",
        "                    face_detect_count = face_detect_count + 1\n",
        "            else:\n",
        "                print('image : ' + input_data_path + \"/\" + file_name + ':NoFace') \n",
        "\n",
        "print(\"顔画像の切り取り作業、正常に動作しました。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaDw_C-GPpd3",
        "colab_type": "text"
      },
      "source": [
        "<h2>\n",
        "<span id=\"ステップ４不要な画像を削除自力で\" class=\"fragment\"></span>・不要な画像を削除（自力で）</h2>\n",
        "ご認識で顔でない部分の写真もあるので、全て削除。手動で"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhaiqPo-P-ak",
        "colab_type": "text"
      },
      "source": [
        "<h2>\n",
        "<span id=\"ステップ５画像処理をして画像の水増し\" class=\"fragment\"></span>・画像処理をして、画像の水増し</h2>\n",
        "\n",
        "<p>ここまでで、写真を保存して、顔写真の切り出しまで保存できた。<br>\n",
        "また、不要な写真も除去した。<br>\n",
        "ここから、画像を<strong>閾値処理、ぼかし処理、回転処理</strong>をして水増しを行います。</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1O-cisUQScA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "from scipy import ndimage\n",
        "\"\"\"\n",
        "ディレクトリから画像を読み込んで回転、ぼかし、閾値処理をしてFaceEditedディレクトリに保存する.\n",
        "\"\"\"\n",
        "\n",
        "input_dir = groupdir + \"/face/\"\n",
        "FaceName =  os.listdir(input_dir)\n",
        "print(FaceName)\n",
        "ImgSize=(250,250)\n",
        "\n",
        "for name in FaceName:\n",
        "    print(\"{}の写真を増やします。\".format(name))\n",
        "    in_dir = input_dir + name+\"/*\"\n",
        "    out_dir = groupdir + \"/FaceEdited/\"+name\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    in_jpg=glob.glob(in_dir)\n",
        "    img_file_name_list=os.listdir(input_dir+name)\n",
        "    for i in range(len(in_jpg)):\n",
        "        #print(str(in_jpg[i]))\n",
        "        img = cv2.imread(str(in_jpg[i]))\n",
        "        # 回転\n",
        "        for ang in [-10,0,10]:\n",
        "            img_rot = ndimage.rotate(img,ang)\n",
        "            img_rot = cv2.resize(img_rot,ImgSize)\n",
        "            fileName=os.path.join(out_dir,str(i)+\"_\"+str(ang)+\".jpg\")\n",
        "            cv2.imwrite(str(fileName),img_rot)\n",
        "            # 閾値\n",
        "            img_thr = cv2.threshold(img_rot, 100, 255, cv2.THRESH_TOZERO)[1]\n",
        "            fileName=os.path.join(out_dir,str(i)+\"_\"+str(ang)+\"thr.jpg\")\n",
        "            cv2.imwrite(str(fileName),img_thr)\n",
        "            # ぼかし\n",
        "            img_filter = cv2.GaussianBlur(img_rot, (5, 5), 0)\n",
        "            fileName=os.path.join(out_dir,str(i)+\"_\"+str(ang)+\"filter.jpg\")\n",
        "            cv2.imwrite(str(fileName),img_filter)\n",
        "\n",
        "print(\"画像の水増しに大成功しました！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l774wRQLYYJ8",
        "colab_type": "text"
      },
      "source": [
        "<h2>\n",
        "<span id=\"ステップ６トレーニングデータテストデータの準備\" class=\"fragment\"></span>・トレーニングデータ、テストデータの準備</h2>\n",
        "\n",
        "<p>FaceEditedフォルダにある画像の２割をテストフォルダに移行します。</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNDLZ3GKYY0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2割をテストデータに移行\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import os\n",
        "\n",
        "input_dir = groupdir + \"/FaceEdited/\"\n",
        "FaceName =  os.listdir(input_dir)\n",
        "print(FaceName)\n",
        "\n",
        "\n",
        "for name in FaceName:\n",
        "    in_dir = groupdir + \"/FaceEdited/\"+name+\"/*\"\n",
        "    in_jpg=glob.glob(in_dir)\n",
        "    img_file_name_list=os.listdir(groupdir + \"/FaceEdited/\"+name+\"/\")\n",
        "    #img_file_name_listをシャッフル、そのうち2割をtest_imageディテクトリに入れる\n",
        "    random.shuffle(in_jpg)\n",
        "    os.makedirs(groupdir + '/test/' + name, exist_ok=True)\n",
        "    for t in range(len(in_jpg)//5):\n",
        "        shutil.move(str(in_jpg[t]), groupdir + \"/test/\"+name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-HhNCKwETbU",
        "colab_type": "text"
      },
      "source": [
        "<h1>\n",
        "<span id=\"モデルの構築\" class=\"fragment\"></span>・モデルの構築</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLnXPZDHE1Hu",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"ライブラリの読み込み\" class=\"fragment\"></span>・ライブラリの読み込み</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmMVaN76CuYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Activation, Dropout, Flatten, Dense, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efwNZFcPKTBv",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"事前に設定するパラメータ\" class=\"fragment\"></span>・事前に設定するパラメータ　＊編集</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXlhe75sFCXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Group_name = \"YOUR_GROUP_NAME\" # グループ名を入力\n",
        "data_path = '/content/gdrive/My Drive/Colab Notebooks/' + Group_name\n",
        "\n",
        "# トレーニング用とバリデーション用の画像格納先（パスは自分で設定してください）\n",
        "train_data_dir = data_path + '/FaceEdited/'\n",
        "validation_data_dir = data_path + '/test/'\n",
        "\n",
        "# 分類するクラス\n",
        "classes = os.listdir(train_data_dir)  # [\"名前1\",\"名前2\",\"名前3\"]\n",
        "print(classes)\n",
        "nb_classes = len(classes)\n",
        "#画像の大きさを設定\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "\n",
        "nb_train_samples = 0\n",
        "nb_validation_samples = 0\n",
        "\n",
        "# 画像数 計算\n",
        "for name in classes:\n",
        "  print(name)\n",
        "  nb_train_samples += len(os.listdir(train_data_dir + name))\n",
        "  nb_validation_samples += len(os.listdir(validation_data_dir + name))\n",
        "\n",
        "#トレーニングデータ用の画像数\n",
        "print(\"train data num : \" + str(nb_train_samples))\n",
        "#バリデーション用の画像数\n",
        "print(\"validation data num : \" + str(nb_validation_samples))\n",
        "#バッチサイズ\n",
        "batch_size = 100\n",
        "#エポック数\n",
        "nb_epoch = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPtxN-5UPJpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# トレーンング用、バリデーション用データを生成するジェネレータ作成\n",
        "train_datagen = ImageDataGenerator(\n",
        "  rescale=1.0 / 255,\n",
        "  #すでに画像の水増し済みの方は、下記２行は必要ありません。\n",
        "  #zoom_range=0.2,\n",
        "  #horizontal_flip=True\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "  train_data_dir,\n",
        "  target_size=(img_width, img_height),\n",
        "  color_mode='rgb',\n",
        "  classes=classes,\n",
        "  class_mode='categorical',\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "  validation_data_dir,\n",
        "  target_size=(img_width, img_height),\n",
        "  color_mode='rgb',\n",
        "  classes=classes,\n",
        "  class_mode='categorical',\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17oacF81P7oJ",
        "colab_type": "text"
      },
      "source": [
        "<h1>\n",
        "<span id=\"ステップ3-モデルの構築\" class=\"fragment\"></span>・モデルの構築</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLszAfkeQHfH",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"vgg16とは\" class=\"fragment\"></span><a href=\"#vgg16%E3%81%A8%E3%81%AF\"><i class=\"fa fa-link\"></i></a>・VGG16とは</h3>\n",
        "\n",
        "<p><strong>VGG16</strong>は、畳み込み13層とフル結合3層の計16層から成る畳み込みニューラルネットワークです。ImageNetと呼ばれる大規模な画像データセットを使って訓練したモデルで、<strong>1000種類の画像</strong>を分類することができます。<br>\n",
        "Kerasでは、ImageNetで事前学習した重みを利用可能なVGG16モデルを、下記のコードで簡単に使うことができます。</p>\n",
        "\n",
        "<div class=\"code-frame\" data-lang=\"python\"><div class=\"highlight\"><pre><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">applications</span><span class=\"p\">.</span><span class=\"n\">vgg16</span><span class=\"p\">.</span><span class=\"n\">VGG16</span><span class=\"p\">(</span><span class=\"n\">include_top</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">weights</span><span class=\"o\">=</span><span class=\"s\">'imagenet'</span><span class=\"p\">,</span> <span class=\"n\">input_tensor</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">pooling</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">classes</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">)</span>\n",
        "</pre></div></div>\n",
        "\n",
        "<p>他にも様々なモデルが利用可能です。<br>\n",
        "<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2Fa0586fce-6231-e593-3e7c-426173f034a9.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=f644fdb99793a1b552b5f7d8a9b4dfc5\" target=\"_blank\" rel=\"nofollow noopener\"><img width=\"538\" alt=\"スクリーンショット 2019-01-12 12.02.44.png\" src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2Fa0586fce-6231-e593-3e7c-426173f034a9.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=f644fdb99793a1b552b5f7d8a9b4dfc5\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/322279/a0586fce-6231-e593-3e7c-426173f034a9.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2Fa0586fce-6231-e593-3e7c-426173f034a9.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=411f4acbc02f2451385046b2271bafe9 1x\" loading=\"lazy\"></a></p>\n",
        "\n",
        "<p>詳しくはリンクを参照してください。<br>\n",
        "<a href=\"https://keras.io/ja/applications/#vgg16\" rel=\"nofollow noopener\" target=\"_blank\">Keras Documentation</a></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hDeJlv4QWCm",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"vgg16をfine-tuning\" class=\"fragment\"></span>・VGG16をFine-tuningする</h3>\n",
        "\n",
        "<p><strong>Fine-tuning</strong>は、学習済モデルを、<strong>重みデータを一部再学習</strong>して特徴量抽出機として利用します<br>\n",
        "<strong>転移学習</strong>は、学習済みモデルを、<strong>重みデータは変更せず</strong>に特徴量抽出機として利用します<br>\n",
        "<strong>Fine-tuning</strong>のメリットは、ニューラルネットの重みを<font color=\"Red\"><strong>少量のデータ</strong></font>で再調整することができて、かつ、高い精度を出せることです。</p>\n",
        "\n",
        "<p>今回は下の画像の<br>\n",
        "<font color=\"Blue\"><strong>青い層</strong></font><strong>：重みデータは変更しない(frozen)</strong><br>\n",
        "<font color=\"Gold\"><strong>黄色い層</strong></font><strong>：重みデータを再学習(Fine-tuning)</strong><br>\n",
        "<font color=\"ForestGreen\"><strong>緑の層</strong></font><strong>：フル結合層（Fine-tuning）</strong></p>\n",
        "\n",
        "<div>\n",
        " <a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F42db31cc-adad-32e4-2c85-f27c319bc257.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=ba7df877ef70cf9c3817312a187a4572\" target=\"_blank\" rel=\"nofollow noopener\"><img width=\"30%\" alt=\"20170110205228.png\" src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F42db31cc-adad-32e4-2c85-f27c319bc257.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=ba7df877ef70cf9c3817312a187a4572\" data-canonical-src=\"https://qiita-image-store.s3.amazonaws.com/0/322279/42db31cc-adad-32e4-2c85-f27c319bc257.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F322279%2F42db31cc-adad-32e4-2c85-f27c319bc257.png?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=c931673b8f12e2418ae6803a86f2bf05 1x\" loading=\"lazy\"></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUVToy_jQ7LY",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"kerasでモデルを構築\" class=\"fragment\"></span>・kerasでモデルを構築</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_KDweOLP6Az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG16のロード。FC層は不要なので include_top=False\n",
        "input_tensor = Input(shape=(img_width, img_height, 3))\n",
        "vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
        "\n",
        "# VGG16の図の緑色の部分（FC層）の作成\n",
        "top_model = Sequential()\n",
        "top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
        "top_model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
        "top_model.add(BatchNormalization())\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "# VGG16とFC層を結合してモデルを作成（完成図が上の図）\n",
        "vgg_model = Model(inputs=vgg16.input, outputs=top_model(vgg16.output))\n",
        "\n",
        "# VGG16の図の青色の部分は重みを固定（frozen）\n",
        "for layer in vgg_model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 多クラス分類を指定\n",
        "vgg_model.compile(loss='categorical_crossentropy',\n",
        "          optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA9MspZ7W2R6",
        "colab_type": "text"
      },
      "source": [
        "<h1>\n",
        "<span id=\"ステップ4-モデルの学習評価\" class=\"fragment\"></span>・モデルの学習・評価</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmHI5Va4W-wa",
        "colab_type": "text"
      },
      "source": [
        "<h3>\n",
        "<span id=\"モデルの学習\" class=\"fragment\"></span>・モデルの学習</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSkac6M2XLi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine-tuning\n",
        "history = vgg_model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=nb_train_samples / batch_size +1, # batch_size,\n",
        "        epochs=nb_epoch,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=nb_validation_samples / batch_size + 1 # batch_size\n",
        "        )\n",
        "\n",
        "# モデルを保存\n",
        "weights_dir = '/content/gdrive/My Drive/Colab Notebooks/weights/'\n",
        "models_dir = '/content/gdrive/My Drive/Colab Notebooks/models/'\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.mkdir(weights_dir)\n",
        "if not os.path.exists(models_dir):\n",
        "    os.mkdir(models_dir)\n",
        "\n",
        "vgg_model.save_weights(os.path.join(weights_dir, 'Final.h5'))\n",
        "\n",
        "vgg_model.save(os.path.join(models_dir, \"VGGtake1.h5\"))\n",
        "\n",
        "vgg_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOXGMB9IdKxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習結果を描写\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history)\n",
        "\n",
        "#acc, val_accのプロット\n",
        "plt.plot(history.history[\"accuracy\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend(loc=\"best\")\n",
        "#Final.pngという名前で、結果を保存\n",
        "plts_dir = '/content/gdrive/My Drive/Colab Notebooks/plt/'\n",
        "if not os.path.exists(plts_dir):\n",
        "    os.mkdir(plts_dir)\n",
        "plt.savefig('/content/gdrive/My Drive/Colab Notebooks/plt/Final_acc.png')\n",
        "plt.show()\n",
        "\n",
        "#loss, val_lossのプロット\n",
        "plt.plot(history.history[\"loss\"], label=\"loss\", ls=\"-\", marker=\"o\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\", ls=\"-\", marker=\"x\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend(loc=\"best\")\n",
        "#Final.pngという名前で、結果を保存\n",
        "plts_dir = '/content/gdrive/My Drive/Colab Notebooks/plt/'\n",
        "if not os.path.exists(plts_dir):\n",
        "    os.mkdir(plts_dir)\n",
        "plt.savefig('/content/gdrive/My Drive/Colab Notebooks/plt/Final_loss.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV0L0TW0e7qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テスト用のコード\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "\n",
        "# 画像を読み込んで予測する\n",
        "def img_predict(filename):\n",
        "  # 画像を読み込んで4次元テンソルへ変換\n",
        "  img = image.load_img(filename, target_size=(img_height, img_width))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  # 学習時にImageDataGeneratorのrescaleで正規化したので同じ処理が必要\n",
        "  # これを忘れると結果がおかしくなるので注意\n",
        "  x = x / 255.0   \n",
        "  #表示\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "  # 指数表記を禁止にする\n",
        "  np.set_printoptions(suppress=True)\n",
        "\n",
        "  #画像の人物を予測    \n",
        "  pred = vgg_model.predict(x)[0]\n",
        "  #結果を表示する\n",
        "  print( classes[0] + ': 0, ' + classes[1] + ': 1, ' + classes[2] + ': 2') #人数分\n",
        "  print(pred*100)\n",
        "  prelabel = np.argmax(pred)\n",
        "  print(\" >>> \"+ classes[prelabel])\n",
        "\n",
        "# フォルダ内ファイルを変数に格納(ディレクトリも格納)\n",
        "pattern = \".*\\.(jpg|png|bmp)\"\n",
        "files=[]\n",
        "for name in classes:\n",
        "  test_dir = \"/content/gdrive/My Drive/Colab Notebooks/\" + Group_name + \"/face/\" + name\n",
        "  files = [f for f in os.listdir(test_dir) if re.search(pattern, f, re.IGNORECASE)]\n",
        "  print(\"Name :\" + name + \", files :\" + str(files[:2]))\n",
        "\n",
        "  # 集めた画像データから顔が検知されたら、切り取り、保存する。\n",
        "  for filename in files[:2]:\n",
        "    img_predict(test_dir + \"/\" + filename)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}